---
title: "LambdaNetworks: Modeling long-range Interactions without Attention:ICLR 2021"
date: 2021-02-25
categories: DeepLearning
---

<b>기본 개념 </b> <br>

self attention에 대한 대안적인 프레임 워크라고 주장 > lambda layers <br>
self attention은 각 query를 Context에 대한 attention distribution와 연관된 개념이고 <br>
lambda layer은 각 context를 해당되는 query에 적용되는 선형 함수 lambda로 변환(transforms) <br>
이는 context 정보를 self attention는 attention 정보에 의해 처리하고, LambdaNetworks는 선형함수인 lambda로 표현된다. <br><br>

<b> lambda layer 설명 </b> <br><br>

<img src="https://user-images.githubusercontent.com/40360823/109110720-0005e080-777b-11eb-96fa-f7bdc89a2bf4.png" <\img> <br><br>

<b> lambda layer 소스 </b> <br><br>
<img src="https://user-images.githubusercontent.com/40360823/109112453-08abe600-777e-11eb-8cd4-102fd254a5fc.png" <\img> <br><br>
