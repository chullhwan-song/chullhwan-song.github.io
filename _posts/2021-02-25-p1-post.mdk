---
title: "LambdaNetworks: Modeling long-range Interactions without Attention:ICLR 2021"
date: 2021-02-25
categories: DeepLearning
---

<b>기본 개념 </b> <br>

self attention에 대한 대안적인 프레임 워크라고 주장 > lambda layers <br>
self attention은 각 query를 Context에 대한 attention distribution와 연관된 개념이고 <br>
lambda layer은 각 context를 해당되는 query에 적용되는 선형 함수 lambda로 변환(transforms) <br>
이는 context 정보를 self attention는 attention 정보에 의해 처리하고, LambdaNetworks는 선형함수인 lambda <br>

<b> 설명</b> <br>

![image](https://user-images.githubusercontent.com/40360823/109110720-0005e080-777b-11eb-96fa-f7bdc89a2bf4.png) <br>
