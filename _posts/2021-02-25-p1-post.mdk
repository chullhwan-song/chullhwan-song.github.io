---
title: "LambdaNetworks: Modeling long-range Interactions without Attention:ICLR 2021"
date: 2021-02-25
categories: DeepLearning
---

<b>개념자체가 이해가 안간다.</b> <br>

* self attention에 대한 대안적인 프레임 워크라고 주장 > lambda layers <br>
* self attention은 각 query를 Context에 대한 attention distribution와 연관된 개념이고 <br>
* lambda layer은 각 context를 해당되는 query에 적용되는 선형 함수 lambda로 변환(transforms) <br>
* 이는 context 정보를 self attention는 attention 정보에 의해 처리하고, LambdaNetworks는 선형함수인 lambda  <br>
