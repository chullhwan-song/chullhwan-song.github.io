---
title: "LambdaNetworks: Modeling long-range Interactions without Attention:ICLR 2021"
date: 2021-02-25
categories: DeepLearning
---

## 공부하기

* self attention에 대한 대안적인 프레임 워크라고 주장 > lambda layers 
* self attention은 각 query를 Context에 대한 attention distribution와 연관된 개념이고
* lambda layer은 각 context를 해당 query에 적용되는 선형 함수 lambda로 변환(transforms)
   

테스트다. <br>
